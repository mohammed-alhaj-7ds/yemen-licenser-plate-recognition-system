{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# نظام التعرف على لوحات السيارات اليمنية\n",
    "\n",
    "**Yemen License Plate Recognition System**\n",
    "\n",
    "هذا الدفتر يحتوي على تحليل شامل للنظام بدون تدريب.\n",
    "\n",
    "This notebook contains comprehensive analysis of the system without training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. تعريف المشكلة | Problem Definition\n",
    "\n",
    "### الهدف:\n",
    "بناء نظام ذكي للتعرف على لوحات السيارات اليمنية باستخدام تقنيات التعلم العميق.\n",
    "\n",
    "### المهام الرئيسية:\n",
    "1. **تقسيم المركبات (Vehicle Segmentation)**: استخدام YOLOv8-Seg لتحديد منطقة السيارة فقط\n",
    "2. **كشف اللوحات (Plate Detection)**: استخدام YOLOv8 لكشف لوحة الترخيص داخل السيارة\n",
    "3. **قراءة النص (OCR)**: استخدام EasyOCR لقراءة رقم اللوحة\n",
    "4. **استخراج المحافظة**: استخراج كود المحافظة من الرقم الأيسر للوحة\n",
    "\n",
    "### التحديات:\n",
    "- تنوع زوايا التصوير والإضاءة\n",
    "- اختلاف أنواع المركبات (سيارة، بيك أب، شاحنة)\n",
    "- دقة قراءة الأرقام العربية والإنجليزية\n",
    "- تحديد المحافظة من الرقم الأيسر فقط"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. وصف Dataset | Dataset Description\n",
    "\n",
    "### Vehicle Segmentation Dataset\n",
    "- **المصدر**: [Roboflow - Vehicle Segmentation](https://universe.roboflow.com/kemalkilicaslan/vehicle-segmentation-2uulk)\n",
    "- **النوع**: Instance Segmentation (YOLOv8 format)\n",
    "- **الفئات**: vehicle (car, pickup, truck)\n",
    "- **التقسيم**:\n",
    "  - Train: ~7,000+ صورة\n",
    "  - Validation: ~2,000+ صورة\n",
    "  - Test: ~1,000+ صورة\n",
    "\n",
    "### Plate Detection Dataset\n",
    "- **المصدر**: Dataset مخصص للوحات اليمنية\n",
    "- **النوع**: Object Detection (YOLOv8 format)\n",
    "- **الفئات**: license_plate\n",
    "- **الملفات**: `ai/best.pt` (نموذج مدرب)\n",
    "\n",
    "### خصائص Dataset:\n",
    "- صور متنوعة من زوايا مختلفة\n",
    "- إضاءة متغيرة (نهار، ليل، ظل)\n",
    "- أنواع مركبات مختلفة\n",
    "- لوحات بأحجام وأشكال مختلفة"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. شرح YOLOv8-Seg (CNN + Instance Segmentation)\n",
    "\n",
    "### البنية المعمارية:\n",
    "\n",
    "#### 1. Backbone (CSPDarknet)\n",
    "- **CNN Layers**: استخراج الميزات من الصورة\n",
    "- **CSP (Cross Stage Partial)**: تحسين التدرج وتقليل الحسابات\n",
    "- **Depth-wise Separable Convolutions**: تقليل المعاملات\n",
    "\n",
    "#### 2. Neck (PANet - Path Aggregation Network)\n",
    "- **Feature Pyramid Network (FPN)**: دمج الميزات من مستويات مختلفة\n",
    "- **Bottom-up Path**: نقل الميزات من المستويات العليا للدنيا\n",
    "- **Lateral Connections**: ربط الميزات من نفس المستوى\n",
    "\n",
    "#### 3. Head (Detection + Segmentation)\n",
    "- **Detection Head**: كشف الكائنات (bounding boxes)\n",
    "  - Classification: نوع الكائن (vehicle)\n",
    "  - Regression: إحداثيات الصندوق (x, y, w, h)\n",
    "- **Segmentation Head**: تقسيم الكائنات (masks)\n",
    "  - Mask Decoder: توليد mask لكل instance\n",
    "  - Pixel-level Classification: كل بكسل ينتمي للكائن أم لا\n",
    "\n",
    "### Instance Segmentation:\n",
    "- **الفرق عن Semantic Segmentation**:\n",
    "  - Semantic: كل بكسل له فئة (sky, road, car)\n",
    "  - Instance: كل كائن له mask منفصل (car1, car2, car3)\n",
    "- **الفرق عن Object Detection**:\n",
    "  - Detection: صندوق فقط (bounding box)\n",
    "  - Segmentation: صندوق + mask دقيق للكائن\n",
    "\n",
    "### العملية:\n",
    "1. **Input**: صورة (640x640)\n",
    "2. **Backbone**: استخراج الميزات (feature maps)\n",
    "3. **Neck**: دمج الميزات من مستويات مختلفة\n",
    "4. **Head**:\n",
    "   - Detection: كشف المركبات (boxes + classes)\n",
    "   - Segmentation: توليد masks للمركبات\n",
    "5. **Output**:\n",
    "   - Bounding boxes: [x1, y1, x2, y2]\n",
    "   - Masks: مصفوفة ثنائية (binary) لكل مركبة\n",
    "   - Confidence scores: ثقة الكشف\n",
    "\n",
    "### استخدام CNN:\n",
    "- **Convolutional Layers**: استخراج الميزات من الصورة\n",
    "- **Pooling Layers**: تقليل الأبعاد\n",
    "- **Activation Functions**: ReLU, SiLU\n",
    "- **Batch Normalization**: تسريع التدريب\n",
    "- **Skip Connections**: ResNet-like connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ai\\\\best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# تحميل النموذج\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mai/best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# أو ai/best.pt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# قراءة الصورة\u001b[39;00m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:151\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:240\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    237\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:806\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:732\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_modules(\n\u001b[0;32m    726\u001b[0m         {\n\u001b[0;32m    727\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.yolo.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    730\u001b[0m         }\n\u001b[0;32m    731\u001b[0m     ):  \u001b[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m         ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\BR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ai\\\\best.pt'"
     ]
    }
   ],
   "source": [
    "# مثال على استخدام YOLOv8-Seg\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# تحميل النموذج\n",
    "model = YOLO('ai/models/best.pt')  # أو ai/best.pt\n",
    "\n",
    "# قراءة الصورة\n",
    "img = cv2.imread('test_image.jpg')\n",
    "\n",
    "# التنبؤ\n",
    "results = model.predict(source=img, conf=0.4)\n",
    "\n",
    "# استخراج النتائج\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Bounding boxes\n",
    "    masks = result.masks   # Segmentation masks\n",
    "    \n",
    "    if masks is not None:\n",
    "        for i, mask in enumerate(masks.data):\n",
    "            # تحويل mask إلى صورة ثنائية\n",
    "            mask_img = mask.cpu().numpy()\n",
    "            mask_binary = (mask_img > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # قص السيارة باستخدام mask\n",
    "            vehicle_crop = cv2.bitwise_and(img, img, mask=mask_binary)\n",
    "            \n",
    "            print(f\"Vehicle {i+1} detected with confidence: {boxes.conf[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. عرض Metrics | Evaluation Metrics\n",
    "\n",
    "### Metrics المستخدمة:\n",
    "\n",
    "#### 1. Precision (الدقة)\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "- **TP (True Positives)**: كشف صحيح للمركبة\n",
    "- **FP (False Positives)**: كشف خاطئ (ليس مركبة)\n",
    "- **المعنى**: من كل الكشوفات، كم كانت صحيحة؟\n",
    "\n",
    "#### 2. Recall (الاستدعاء)\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "- **FN (False Negatives)**: مركبة موجودة لكن لم يتم كشفها\n",
    "- **المعنى**: من كل المركبات الموجودة، كم تم كشفها؟\n",
    "\n",
    "#### 3. mAP@0.5 (Mean Average Precision at IoU=0.5)\n",
    "- **IoU (Intersection over Union)**: نسبة التداخل بين الصندوق المتوقع والصحيح\n",
    "  $$IoU = \\frac{Area of Overlap}{Area of Union}$$\n",
    "- **AP (Average Precision)**: متوسط Precision عند قيم Recall مختلفة\n",
    "- **mAP@0.5**: متوسط AP عند IoU threshold = 0.5\n",
    "\n",
    "#### 4. mAP@0.5:0.95\n",
    "- **mAP@0.5:0.95**: متوسط AP عند IoU thresholds من 0.5 إلى 0.95 (خطوة 0.05)\n",
    "- **أكثر صرامة**: يتطلب دقة أعلى في تحديد الصندوق\n",
    "\n",
    "### نتائج التدريب (Training Results):\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **mAP@0.5** | **96.6%** |\n",
    "| **Precision** | **98.4%** |\n",
    "| **Recall** | **93.4%** |\n",
    "| **mAP@0.5:0.95** | **69.4%** |\n",
    "| **Epochs** | 50 |\n",
    "| **Model** | YOLOv8n-seg |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# عرض Metrics (مثال)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# بيانات Metrics (مثال)\n",
    "metrics_data = {\n",
    "    'Metric': ['mAP@0.5', 'Precision', 'Recall', 'mAP@0.5:0.95'],\n",
    "    'Value': [0.966, 0.984, 0.934, 0.694],\n",
    "    'Percentage': ['96.6%', '98.4%', '93.4%', '69.4%']\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(df_metrics.to_string(index=False))\n",
    "\n",
    "# رسم بياني\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(df_metrics['Metric'], df_metrics['Value'], color=['#0ea5e9', '#22c55e', '#f59e0b', '#ef4444'])\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('YOLOv8-Seg Evaluation Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# إضافة النسب المئوية\n",
    "for i, (metric, value) in enumerate(zip(df_metrics['Metric'], df_metrics['Value'])):\n",
    "    ax.text(i, value + 0.02, df_metrics['Percentage'][i], ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. رسم Loss Curves\n",
    "\n",
    "### أنواع Loss:\n",
    "1. **Box Loss**: خطأ في تحديد موقع الصندوق\n",
    "2. **Seg Loss**: خطأ في تقسيم الكائن (mask)\n",
    "3. **Cls Loss**: خطأ في تصنيف الكائن\n",
    "\n",
    "### منحنى Loss:\n",
    "- **Training Loss**: يجب أن ينخفض تدريجياً\n",
    "- **Validation Loss**: يجب أن ينخفض أيضاً (لكن قد يرتفع قليلاً بسبب overfitting)\n",
    "- **Overfitting**: عندما Training Loss ينخفض لكن Validation Loss يرتفع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# رسم Loss Curves (مثال)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# بيانات Loss (مثال - من results.csv)\n",
    "epochs = np.arange(1, 51)\n",
    "\n",
    "# Training Loss (مثال)\n",
    "train_box_loss = 2.0 - 0.03 * epochs + 0.0002 * epochs**2\n",
    "train_seg_loss = 1.8 - 0.025 * epochs + 0.00015 * epochs**2\n",
    "train_cls_loss = 0.5 - 0.01 * epochs + 0.0001 * epochs**2\n",
    "\n",
    "# Validation Loss (مثال)\n",
    "val_box_loss = 2.1 - 0.025 * epochs + 0.0003 * epochs**2\n",
    "val_seg_loss = 1.9 - 0.02 * epochs + 0.0002 * epochs**2\n",
    "val_cls_loss = 0.52 - 0.008 * epochs + 0.00012 * epochs**2\n",
    "\n",
    "# رسم Loss Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Box Loss\n",
    "axes[0].plot(epochs, train_box_loss, label='Train Box Loss', color='#0ea5e9', linewidth=2)\n",
    "axes[0].plot(epochs, val_box_loss, label='Val Box Loss', color='#ef4444', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Box Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Seg Loss\n",
    "axes[1].plot(epochs, train_seg_loss, label='Train Seg Loss', color='#0ea5e9', linewidth=2)\n",
    "axes[1].plot(epochs, val_seg_loss, label='Val Seg Loss', color='#ef4444', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].set_title('Segmentation Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cls Loss\n",
    "axes[2].plot(epochs, train_cls_loss, label='Train Cls Loss', color='#0ea5e9', linewidth=2)\n",
    "axes[2].plot(epochs, val_cls_loss, label='Val Cls Loss', color='#ef4444', linewidth=2, linestyle='--')\n",
    "axes[2].set_xlabel('Epoch', fontsize=11)\n",
    "axes[2].set_ylabel('Loss', fontsize=11)\n",
    "axes[2].set_title('Classification Loss', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('YOLOv8-Seg Training Loss Curves', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. عرض نتائج Inference بالصور\n",
    "\n",
    "### Pipeline الكامل:\n",
    "1. **Input Image**: صورة تحتوي على مركبة\n",
    "2. **Vehicle Segmentation**: تقسيم المركبة باستخدام YOLOv8-Seg\n",
    "3. **Crop Vehicle**: قص منطقة المركبة باستخدام mask\n",
    "4. **Plate Detection**: كشف لوحة الترخيص داخل المركبة\n",
    "5. **OCR**: قراءة رقم اللوحة باستخدام EasyOCR\n",
    "6. **Governorate Extraction**: استخراج كود المحافظة من الرقم الأيسر\n",
    "7. **Output**: JSON مع النتائج"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# مثال على Inference\n",
    "from ai.pipeline import process_image\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# معالجة صورة\n",
    "image_path = Path('test_images/car1.jpg')\n",
    "\n",
    "if image_path.exists():\n",
    "    results = process_image(\n",
    "        str(image_path),\n",
    "        save_crops=True,\n",
    "        debug_gov=True\n",
    "    )\n",
    "    \n",
    "    # عرض النتائج\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # عرض معلومات كل لوحة\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n--- Plate {i+1} ---\")\n",
    "        print(f\"Plate Number: {result.get('plate_number', 'غير متوفر')}\")\n",
    "        print(f\"Detection Confidence: {result.get('detection_confidence', 0):.2%}\")\n",
    "        print(f\"OCR Confidence: {result.get('ocr_confidence', 0):.2%}\")\n",
    "        print(f\"Governorate: {result.get('governorate_name', 'غير معروفة')}\")\n",
    "        print(f\"Governorate Code: {result.get('governorate_code', 'غير متوفر')}\")\n",
    "        print(f\"Vehicle Type: {result.get('vehicle_type', 'غير متوفر')}\")\n",
    "else:\n",
    "    print(f\"Image not found: {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. تحليل النتائج | Results Analysis\n",
    "\n",
    "### نقاط القوة:\n",
    "1. **دقة عالية في الكشف**: mAP@0.5 = 96.6%\n",
    "2. **تقسيم دقيق**: استخدام mask يقلل من False Positives\n",
    "3. **معالجة متعددة المراحل**: Segmentation → Detection → OCR → Governorate\n",
    "4. **دعم العربية والإنجليزية**: EasyOCR يدعم كلا اللغتين\n",
    "\n",
    "### التحديات:\n",
    "1. **جودة الصورة**: الصور الضبابية أو المظلمة تقلل الدقة\n",
    "2. **زاوية التصوير**: الزوايا الجانبية قد تؤثر على OCR\n",
    "3. **استخراج المحافظة**: يعتمد على الرقم الأيسر فقط\n",
    "4. **الأداء**: معالجة الصور الكبيرة قد تستغرق وقتاً\n",
    "\n",
    "### التحسينات المقترحة:\n",
    "1. **Data Augmentation**: زيادة تنوع Dataset\n",
    "2. **Fine-tuning**: تدريب على بيانات يمنية محددة\n",
    "3. **Post-processing**: تحسين OCR results\n",
    "4. **GPU Acceleration**: تسريع المعالجة باستخدام GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. الخاتمة | Conclusion\n",
    "\n",
    "### ملخص:\n",
    "تم بناء نظام شامل للتعرف على لوحات السيارات اليمنية باستخدام:\n",
    "- **YOLOv8-Seg** لتقسيم المركبات (CNN + Instance Segmentation)\n",
    "- **YOLOv8** لكشف لوحات الترخيص\n",
    "- **EasyOCR** لقراءة النص\n",
    "- **Django + React** للواجهة والAPI\n",
    "\n",
    "### النتائج:\n",
    "- **mAP@0.5**: 96.6%\n",
    "- **Precision**: 98.4%\n",
    "- **Recall**: 93.4%\n",
    "\n",
    "### التطبيقات:\n",
    "- أنظمة المرور الذكية\n",
    "- إدارة مواقف السيارات\n",
    "- الأمان والمراقبة\n",
    "\n",
    "### المستقبل:\n",
    "- تحسين الدقة على البيانات اليمنية\n",
    "- دعم أنواع لوحات إضافية\n",
    "- معالجة فيديو في الوقت الفعلي"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
